{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "def keep_only_letters(text):\n",
    "    pattern = re.compile('[^a-zA-Z]')\n",
    "    result = re.sub(pattern, '', text)\n",
    "    return result\n",
    "\n",
    "\n",
    "def clean(text):\n",
    "    text = ''.join(text).lower()\n",
    "    text = keep_only_letters(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def make_temp_cols(dataframe):\n",
    "    df = dataframe.copy()\n",
    "    for col in ['prompt', 'response_a', 'response_b']:\n",
    "        df[col+'_temp'] = df[col].apply(clean)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('lmsys/chatbot_arena_conversations')\n",
    "ds = ds['train']\n",
    "external = ds.to_pandas()\n",
    "\n",
    "train_ds = load_dataset('lmsys/lmsys-arena-human-preference-55k')\n",
    "train_ds = train_ds['train']\n",
    "train_ds = train_ds.to_pandas()\n",
    "\n",
    "train_ds = train_ds[train_ds['response_a'] != '[null]']\n",
    "train_ds = train_ds[train_ds['response_b'] != '[null]']\n",
    "\n",
    "train_ds[\"prompt\"] = train_ds.prompt.map(lambda x: eval(x))\n",
    "train_ds[\"response_a\"] = train_ds.response_a.map(lambda x: eval(x.replace(\"null\", \"''\")))\n",
    "train_ds[\"response_b\"] = train_ds.response_b.map(lambda x: eval(x.replace(\"null\", \"''\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "results = []\n",
    "for i in tqdm(range(len(external))):\n",
    "    sample = external.iloc[i].to_dict()\n",
    "    prompts = [conv['content'] for conv in sample['conversation_a'] if conv['role'] == 'user']\n",
    "    response_a = [conv['content'] for conv in sample['conversation_a'] if conv['role'] == 'assistant']\n",
    "    response_b = [conv['content'] for conv in sample['conversation_b'] if conv['role'] == 'assistant']\n",
    "\n",
    "    results.append({\n",
    "        'model_a': sample['model_a'],\n",
    "        'model_b': sample['model_b'],\n",
    "        'prompt': prompts,\n",
    "        'response_a': response_a,\n",
    "        'response_b': response_b,\n",
    "        'winner_model_a': 1 if sample['winner'] == 'model_a' else 0,\n",
    "        'winner_model_b': 1 if sample['winner'] == 'model_b' else 0,\n",
    "        'winner_tie': 1 if sample['winner'] == 'tie' or sample['winner'] == 'tie (bothbad)' else 0,\n",
    "    })\n",
    "\n",
    "external = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external = make_temp_cols(external)\n",
    "train_ds = make_temp_cols(train_ds)\n",
    "\n",
    "external['text'] = external['prompt_temp'] + external['response_a_temp'] + external['response_b_temp']\n",
    "train_ds['text'] = train_ds['prompt_temp'] + train_ds['response_a_temp'] + train_ds['response_b_temp']\n",
    "external['text'] = external['text'].apply(lambda x: ''.join(sorted(x)))\n",
    "train_ds['text'] = train_ds['text'].apply(lambda x: ''.join(sorted(x)))\n",
    "external = external[~external['text'].isin(train_ds['text'].values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external.to_parquet('data/additional/additional_data_v6.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('openai/webgpt_comparisons')\n",
    "ds = ds['train']\n",
    "external = ds.to_pandas()\n",
    "external.drop(['quotes_0', 'tokens_0', 'quotes_1', 'tokens_1'], axis=1, inplace=True)\n",
    "\n",
    "train_ds = load_dataset('lmsys/lmsys-arena-human-preference-55k')\n",
    "train_ds = train_ds['train']\n",
    "train_ds = train_ds.to_pandas()\n",
    "\n",
    "train_ds = train_ds[train_ds['response_a'] != '[null]']\n",
    "train_ds = train_ds[train_ds['response_b'] != '[null]']\n",
    "\n",
    "train_ds[\"prompt\"] = train_ds.prompt.map(lambda x: eval(x))\n",
    "train_ds[\"response_a\"] = train_ds.response_a.map(lambda x: eval(x.replace(\"null\", \"''\")))\n",
    "train_ds[\"response_b\"] = train_ds.response_b.map(lambda x: eval(x.replace(\"null\", \"''\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external['prompt'] = external['question'].apply(lambda x: x['full_text'])\n",
    "external.drop('question', axis=1, inplace=True)\n",
    "external.rename(columns={'answer_0': 'response_a', 'answer_1': 'response_b'}, inplace=True)\n",
    "\n",
    "external = external[(external['response_a'] != '') & (external['response_b'] != '')]\n",
    "\n",
    "external['winner_model_a'] = 0\n",
    "external['winner_model_b'] = 0\n",
    "external['winner_tie'] = 0\n",
    "\n",
    "external.loc[external['score_0'] > external['score_1'], 'winner_model_a'] = 1\n",
    "external.loc[external['score_0'] < external['score_1'], 'winner_model_b'] = 1\n",
    "external.loc[external['score_0'] == external['score_1'], 'winner_tie'] = 1\n",
    "\n",
    "external.drop(['score_0', 'score_1',], axis=1, inplace=True)\n",
    "\n",
    "external['prompt'] = external['prompt'].apply(lambda x: [x])\n",
    "external['response_a'] = external['response_a'].apply(lambda x: [x])\n",
    "external['response_b'] = external['response_b'].apply(lambda x: [x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external.to_parquet('data/additional/additional_data_v8.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
